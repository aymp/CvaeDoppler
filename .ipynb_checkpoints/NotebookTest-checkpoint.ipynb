{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#sys.path.append('/home/manabe/.local/lib/python3.8/site-packages') #GPU使うとき無理やりパス通す←マウントオプションの変更で解決済。\n",
    "#print(sys.version)\n",
    "#print(sys.path)\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import rgb2hex, ListedColormap, Normalize\n",
    "from matplotlib import ticker\n",
    "import codecs\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import subprocess\n",
    "import csv\n",
    "from datetime import datetime\n",
    "#sns.set(style='darkgrid')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "#from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import get_info_from_filelist\n",
    "import model_pool2_125hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['font.family']= 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial']\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['xtick.major.width'] = 1.2\n",
    "plt.rcParams['ytick.major.width'] = 1.2\n",
    "plt.rcParams['axes.linewidth'] = 1.2\n",
    "plt.rcParams['axes.grid']=True\n",
    "plt.rcParams['grid.linestyle']='--'\n",
    "plt.rcParams['grid.linewidth'] = 0.3\n",
    "plt.rcParams[\"legend.markerscale\"] = 2\n",
    "plt.rcParams[\"legend.fancybox\"] = False\n",
    "plt.rcParams[\"legend.framealpha\"] = 1\n",
    "plt.rcParams[\"legend.edgecolor\"] = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, label, datatime_idx, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = data\n",
    "        self.data_num = len(data)\n",
    "        self.label = label\n",
    "        self.datatime_idx = datatime_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            # print(self.data.shape)\n",
    "            # print(self.data[idx].shape)\n",
    "            out_data = self.transform(self.data[idx])\n",
    "            out_label = int(self.label[idx])\n",
    "            out_datatime_idx = int(self.datatime_idx[idx])\n",
    "        else:\n",
    "            out_data = self.data[idx]\n",
    "            out_label =  self.label[idx]\n",
    "            out_datatime_idx = self.datatime_idx[idx]\n",
    "\n",
    "        return out_data, out_label, out_datatime_idx\n",
    "\n",
    "class ModelError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_Cat(q, p, eps=1e-20):\n",
    "    return (q * torch.log(q+eps) - q * torch.log(p+eps)).sum()\n",
    "\n",
    "def KL_Gauss(mu1, logvar1, mu2, logvar2):\n",
    "    #return -0.5 * torch.sum(1 + (logvar1 - logvar2) - (logvar1.exp() - logvar2.exp()) - (mu2 - mu1).pow(2) / logvar2.exp())\n",
    "    #return -0.5 * torch.sum(1 + logvar1 - logvar2 - logvar1.exp()/logvar2.exp() - (mu2-mu1) ** 2 / logvar2.exp())\n",
    "    #return -0.5 * torch.sum(1 + logvar1 - logvar2 - mu1.pow(2) + mu2.pow(2) - logvar1.exp() + logvar2.exp())\n",
    "    return -0.5 * torch.sum(1 + (logvar1 - logvar2) - (((mu2 - mu1).pow(2))) - (logvar1.exp()/logvar2.exp()))\n",
    "\n",
    "def reconst_plot(x, recon_x, dir_out, fs, wavelet_height, wavelet_width, count, mode='train'):\n",
    "    N=wavelet_width\n",
    "    dt=1/fs\n",
    "    for i in range(x.shape[0]):\n",
    "        if count.detach().cpu().numpy()[i] == 0:\n",
    "            fig = plt.figure(figsize=(1.2,1),dpi=300)\n",
    "            ax=fig.add_subplot(111)\n",
    "            t = np.arange(0,N+1)*dt # time array\n",
    "            plt.pcolormesh(t,np.sort(np.arange(0,wavelet_height+1))[::-1], x.detach().cpu().numpy()[i][0], vmin=0,cmap='viridis')\n",
    "            ax.tick_params(bottom=False,left=False,right=False,top=False)\n",
    "            ax.tick_params(labelbottom=False,labelleft=False,labelright=False,labeltop=False)\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_xlabel('Time')\n",
    "            fig.tight_layout()\n",
    "            if mode == 'train':\n",
    "                fig.savefig(dir_out+'train_'+str(count.detach().cpu().numpy()[i])+'.svg')\n",
    "            if mode == 'test':\n",
    "                fig.savefig(dir_out+'test_'+str(count.detach().cpu().numpy()[i])+'.svg')\n",
    "                plt.close()\n",
    "\n",
    "            fig = plt.figure(figsize=(1.2,1),dpi=300)\n",
    "            ax=fig.add_subplot(111)\n",
    "            t = np.arange(0,N+1)*dt # time array\n",
    "            plt.pcolormesh(t,np.sort(np.arange(0,wavelet_height+1))[::-1], recon_x.detach().cpu().numpy()[i][0], vmin=0,cmap='viridis')\n",
    "            ax.tick_params(bottom=False,left=False,right=False,top=False)\n",
    "            ax.tick_params(labelbottom=False,labelleft=False,labelright=False,labeltop=False)\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_xlabel('Time')\n",
    "            fig.tight_layout()\n",
    "            if mode == 'train':\n",
    "                fig.savefig(dir_out+'train_'+str(count.detach().cpu().numpy()[i])+'_recon_x.svg')\n",
    "            if mode == 'test':\n",
    "                fig.savefig(dir_out+'test_'+str(count.detach().cpu().numpy()[i])+'_recon_x.svg')\n",
    "                plt.close()\n",
    "\n",
    "def sample_plot(x):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(10):\n",
    "        plt.subplot(1, 10, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(x.detach().cpu().numpy()[i][0], cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    return -torch.log(-torch.log(U+eps))\n",
    "\n",
    "def gumbel_softmax_sampling(pi, shape, tau, eps=1e-20):\n",
    "    log_pi = torch.log(pi + eps)\n",
    "    g = sample_gumbel(shape).to(device)\n",
    "    y = nn.functional.softmax((log_pi + g)/tau, dim=1)\n",
    "    return y\n",
    "\n",
    "def min_max(x, axis=None):\n",
    "    \"\"\"0-1の範囲に正規化\"\"\"\n",
    "    min = x.min(axis=axis, keepdims=True)\n",
    "    max = x.max(axis=axis, keepdims=True)\n",
    "    result = (x-min)/(max-min)\n",
    "    return result\n",
    "\n",
    "def z_plot(z,t,fig_name,epoch,DIR_OUT,Y_DIM=20):\n",
    "    colors20 = plt.cm.get_cmap('tab20')\n",
    "    fig = plt.figure(num=None, figsize=(4, 3), dpi=300, facecolor='w', edgecolor='k')\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(Y_DIM):\n",
    "        zplot = z[t == i]\n",
    "        num = zplot.shape[0]\n",
    "        c = np.full(num,i)\n",
    "        sc = ax.scatter(zplot[:, 0], zplot[:, 1], s=5, c=c, cmap=colors20,norm=Normalize(vmin=0,vmax=19))\n",
    "    ax.set_xlim(-10,10)\n",
    "    ax.set_ylim(-10,10)\n",
    "    ax.set_aspect('equal',adjustable='box')\n",
    "    cbar = fig.colorbar(sc,aspect=30)\n",
    "    cbar.ax.tick_params(direction='out')\n",
    "    cbar.ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(DIR_OUT+'figs/'+fig_name+'_'+str(epoch)+'.png')\n",
    "    fig.savefig(DIR_OUT+'figs/'+fig_name+'_'+str(epoch)+'.svg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMydataset2D_addNoise(subject_num, test_person_num, filelist, sma_num, wavelet_height, wavelet_width, transform, distance=None, freqs_start=None, freqs_end=None):\n",
    "    \"\"\"ピーク前後で分割した時間軸波形をウェーブレット変換したデータを用いてデータセット作成\"\"\"\n",
    "\n",
    "    LABELs = {}\n",
    "    sub_label = 0\n",
    "\n",
    "    peaks, min_peaks = get_info_from_filelist.count_peaks_detail(filelist, disp=False)\n",
    "    # データかぶらんようにする\n",
    "    min_peaks -= sma_num-1\n",
    "\n",
    "    # 訓練データとテストデータの数を決める（8:2）\n",
    "    numoftrain = int(float(min_peaks * 0.8))\n",
    "    numoftest = min_peaks - numoftrain\n",
    "    print(f\"\\n訓練データ数：{numoftrain}, テストデータ数：{numoftest}\\n\")\n",
    "\n",
    "    for subject in peaks.keys():\n",
    "        if subject not in LABELs:\n",
    "            LABELs[subject] = sub_label\n",
    "            sub_label = sub_label + 1\n",
    "\n",
    "    print(\"########### 被験者とラベル ##########\")\n",
    "    for subject, label in LABELs.items():\n",
    "        print(f\"被験者：{subject},  ラベル：{label}\")\n",
    "\n",
    "    train_datasetlist = np.empty(0)\n",
    "    test_datasetlist = np.empty(0)\n",
    "    train_label = []\n",
    "    test_label = []\n",
    "    train_datatime_idx = []\n",
    "    test_datatime_idx = []\n",
    "    count_train = count_test = 0 #datatime_idx用\n",
    "\n",
    "    # ファイルリスト読み込み\n",
    "    filelist_fp = codecs.open(filelist, 'r')\n",
    "    for _idx, filename in tqdm(enumerate(filelist_fp)):\n",
    "        # ファイル読み込み\n",
    "        split_fpdata = filename.rstrip('\\r\\n').split(',')\n",
    "        #print(f\"Input file: {split_fpdata[0]}\")\n",
    "        fname = split_fpdata[0]\n",
    "    # ----- 色々正規表現で取得\n",
    "        data_subject = re.findall('_([A-Z][a-z]*)_',fname)[0] #被験者 ex)Manabe\n",
    "        tmp = re.findall('/(.*).npy',fname)[0]\n",
    "        data_distance = re.findall('[A-Z][a-z]*_(.*)_rec',tmp)[0] #距離　ex)50cm\n",
    "        #print(data_distance)\n",
    "        # data_pre = int(re.findall('pre_([0-9]+)_post',fname)[0]) #ピーク前何サンプルとってるか\n",
    "        is_with_breathing = True if \"wo_breath\" not in fname else False #呼吸ありか無しか\n",
    "\n",
    "        if distance == None: #距離混合\n",
    "            if is_with_breathing and LABELs[data_subject] < subject_num:\n",
    "                data = np.load(fname)\n",
    "            # ----- Create Train Dataset\n",
    "                data_temp = data[:numoftrain,freqs_start:freqs_end,:]\n",
    "                if freqs_start != None or freqs_end != None:\n",
    "                    for i in range(data_temp.shape[0]):\n",
    "                        data_temp[i] = min_max(data_temp[i])\n",
    "                train_datasetlist = np.append(train_datasetlist, data_temp)\n",
    "                label = [LABELs[data_subject]]*numoftrain\n",
    "                train_label.extend(label)\n",
    "                datatime_idx = list(range(count_train, count_train+numoftrain))\n",
    "                train_datatime_idx.extend(datatime_idx)\n",
    "                count_train += numoftrain\n",
    "\n",
    "            # ----- Create Test Dataset\n",
    "                data_temp = data[numoftrain+sma_num-1:min_peaks+sma_num-1,freqs_start:freqs_end,:]\n",
    "                if freqs_start != None or freqs_end != None:\n",
    "                    for i in range(data_temp.shape[0]):\n",
    "                        data_temp[i] = min_max(data_temp[i])\n",
    "                test_datasetlist = np.append(test_datasetlist, data_temp)\n",
    "                label = [LABELs[data_subject]]*numoftest\n",
    "                test_label.extend(label)\n",
    "                #datatime_idx = list(range(numoftrain+sma_num-1,min_peaks+sma_num-1))\n",
    "                datatime_idx = list(range(count_test,count_test+numoftest))\n",
    "                test_datatime_idx.extend(datatime_idx)\n",
    "                count_test += numoftest\n",
    "\n",
    "            if is_with_breathing and subject_num <= LABELs[data_subject] < subject_num+test_person_num:\n",
    "                data = np.load(fname)\n",
    "                data_temp = data[numoftrain+sma_num-1:min_peaks+sma_num-1,freqs_start:freqs_end,:]\n",
    "                if freqs_start != None or freqs_end != None:\n",
    "                    for i in range(data_temp.shape[0]):\n",
    "                        data_temp[i] = min_max(data_temp[i])\n",
    "                test_datasetlist = np.append(test_datasetlist, data_temp)\n",
    "                label = [subject_num]*numoftest\n",
    "                test_label.extend(label)\n",
    "                #datatime_idx = list(range(numoftrain+sma_num-1,min_peaks+sma_num-1))\n",
    "                datatime_idx = list(range(count_test,count_test+numoftest))\n",
    "                test_datatime_idx.extend(datatime_idx)\n",
    "                count_test += numoftest\n",
    "\n",
    "            \"\"\"\n",
    "            else:\n",
    "            # ----- Create Test Dataset\n",
    "                wo_breath_data = np.load(fname)\n",
    "                npeaks = wo_breath_data.shape[0]\n",
    "                test_datasetlist = np.append(test_datasetlist, wo_breath_data)\n",
    "                label = [LABELs[data_subject]]*npeaks\n",
    "                test_label.extend(label)\n",
    "                datatime_idx = list(range(min_peaks+sma_num-1, min_peaks+sma_num-1+npeaks))\n",
    "                test_datatime_idx.extend(datatime_idx)\"\"\"\n",
    "\n",
    "        else: # 距離指定\n",
    "            if is_with_breathing and data_distance == distance and LABELs[data_subject] < subject_num:\n",
    "                data = np.load(fname)\n",
    "            # ----- Create Train Dataset\n",
    "                data_temp = data[:numoftrain,freqs_start:freqs_end,:]\n",
    "                if freqs_start != None or freqs_end != None:\n",
    "                    for i in range(data_temp.shape[0]):\n",
    "                        data_temp[i] = min_max(data_temp[i])\n",
    "                train_datasetlist = np.append(train_datasetlist, data_temp)\n",
    "                label = [LABELs[data_subject]]*numoftrain\n",
    "                train_label.extend(label)\n",
    "                #datatime_idx = list(range(numoftrain))\n",
    "                datatime_idx = list(range(count_train, count_train+numoftrain))\n",
    "                train_datatime_idx.extend(datatime_idx)\n",
    "                count_train += numoftrain\n",
    "\n",
    "            # ----- Create Test Dataset\n",
    "                \n",
    "                data_temp = data[numoftrain+sma_num-1:min_peaks+sma_num-1,freqs_start:freqs_end,:]\n",
    "                if freqs_start != None or freqs_end != None:\n",
    "                    for i in range(data_temp.shape[0]):\n",
    "                        data_temp[i] = min_max(data_temp[i])\n",
    "                test_datasetlist = np.append(test_datasetlist, data_temp)\n",
    "                label = [LABELs[data_subject]]*numoftest\n",
    "                test_label.extend(label)\n",
    "                #datatime_idx = list(range(numoftrain+sma_num-1,min_peaks+sma_num-1))\n",
    "                datatime_idx = list(range(count_test, count_test+numoftest))\n",
    "                test_datatime_idx.extend(datatime_idx)\n",
    "                count_test += numoftest\n",
    "            \n",
    "            if is_with_breathing and data_distance == distance and subject_num<=LABELs[data_subject]<subject_num+test_person_num:\n",
    "                data = np.load(fname)\n",
    "                data_temp = data[numoftrain+sma_num-1:min_peaks+sma_num-1,freqs_start:freqs_end,:]\n",
    "                if freqs_start != None or freqs_end != None:\n",
    "                    for i in range(data_temp.shape[0]):\n",
    "                        data_temp[i] = min_max(data_temp[i])\n",
    "                test_datasetlist = np.append(test_datasetlist, data_temp)\n",
    "                label = [subject_num]*numoftest\n",
    "                test_label.extend(label)\n",
    "                datatime_idx = list(range(count_test,count_test+numoftest))\n",
    "                test_datatime_idx.extend(datatime_idx)\n",
    "                count_test += numoftest\n",
    "\n",
    "            \"\"\"\n",
    "            elif data_distance == distance:\n",
    "            # ----- Create Test Dataset\n",
    "                wo_breath_data = np.load(fname)\n",
    "                npeaks = wo_breath_data.shape[0]\n",
    "                test_datasetlist = np.append(test_datasetlist, wo_breath_data)\n",
    "                label = [LABELs[data_subject]]*npeaks\n",
    "                test_label.extend(label)\n",
    "                datatime_idx = list(range(min_peaks+sma_num-1, min_peaks+sma_num-1+npeaks))\n",
    "                test_datatime_idx.extend(datatime_idx)\"\"\"\n",
    "    \n",
    "    # 訓練データに「ノイズ」クラスを追加\n",
    "    \n",
    "    num_per_one = int(len(train_label)/subject_num)\n",
    "    noise_dataset = np.random.rand(num_per_one,wavelet_height,wavelet_width)\n",
    "    for i in range(noise_dataset.shape[0]):\n",
    "        noise_dataset[i] = min_max(noise_dataset[i])\n",
    "    label = [subject_num]*num_per_one\n",
    "    datatime_idx = [0]*num_per_one\n",
    "    \n",
    "    train_datasetlist = np.append(train_datasetlist, noise_dataset)\n",
    "    train_label.extend(label)\n",
    "    train_datatime_idx.extend(datatime_idx)\n",
    "\n",
    "    # データ成型\n",
    "    train_datasetlist = np.reshape(train_datasetlist, (-1, wavelet_height, wavelet_width, 1))\n",
    "    test_datasetlist = np.reshape(test_datasetlist, (-1, wavelet_height, wavelet_width, 1))\n",
    "    print(f\"train_datasetlist.shape : {train_datasetlist.shape}\")\n",
    "    print(f\"test_datasetlist.shape : {test_datasetlist.shape}\")\n",
    "\n",
    "    train_label = np.array(train_label)\n",
    "    test_label = np.array(test_label)\n",
    "    print(f\"train_label.shape : {train_label.shape}\")\n",
    "    print(f\"test_label.shape : {test_label.shape}\")\n",
    "    print(f\"train_label : {train_label}\")\n",
    "\n",
    "    train_datatime_idx = np.array(train_datatime_idx)\n",
    "    test_datatime_idx = np.array(test_datatime_idx)\n",
    "    print(f\"train_datatime_idx：{train_datatime_idx.shape}\")\n",
    "    print(f\"test_datatime_idx：{test_datatime_idx.shape}\")\n",
    "\n",
    "    filelist_fp.close()\n",
    "\n",
    "    return MyDataset(train_datasetlist.astype(np.float32), train_label, train_datatime_idx, transform), MyDataset(test_datasetlist.astype(np.float32), test_label, test_datatime_idx, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(K,pz_y,px_z,qy_x,qz_xy,train_loader,optimizer,ALPHA,BETA,device,epoch,DIR_OUT):\n",
    "    pz_y.train()\n",
    "    px_z.train()\n",
    "    qy_x.train()\n",
    "    qz_xy.train()\n",
    "\n",
    "    total_loss = total_recon_loss = total_kl_gauss = total_Xent = 0.0\n",
    "    t_list = []\n",
    "    y_list = []\n",
    "    z_xy_list = []\n",
    "    z_y_list = []\n",
    "\n",
    "    for x, t, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "    # ----- クロスエントロピー項\n",
    "        y = qy_x(x)\n",
    "        Xent = nn.functional.cross_entropy(y, t, reduction=\"sum\")\n",
    "    # ----- 再構性項\n",
    "        t_onehot = torch.eye(K)[t].to(device)\n",
    "        z = qz_xy(x, t_onehot)\n",
    "        recon_x = px_z(z)\n",
    "        recon_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "    # ----- Prior Network Loss\n",
    "        z_y = pz_y(t_onehot)\n",
    "        kl_gauss = KL_Gauss(qz_xy.mu, qz_xy.logvar, pz_y.mu, pz_y.logvar)\n",
    "\n",
    "        loss = recon_loss + kl_gauss*BETA + Xent*ALPHA\n",
    "        total_loss += loss\n",
    "        total_recon_loss += recon_loss\n",
    "        total_kl_gauss += kl_gauss\n",
    "        total_Xent += Xent\n",
    "\n",
    "        y = y.argmax(dim=1).detach().cpu().numpy()\n",
    "        t = t.detach().cpu().numpy()\n",
    "        z = z.detach().cpu().numpy()\n",
    "        z_y = z_y.detach().cpu().numpy()\n",
    "\n",
    "        y_list.extend(y)\n",
    "        t_list.extend(t)\n",
    "        z_xy_list.extend(z)\n",
    "        z_y_list.extend(z_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_gauss = total_kl_gauss / len(train_loader.dataset)\n",
    "    avg_Xent = total_Xent / len(train_loader.dataset)\n",
    "    y_array = np.array(y_list)\n",
    "    t_array = np.array(t_list)\n",
    "    avg_precision = precision_score(t_array,y_array,average='macro',zero_division=0)\n",
    "    avg_recall = recall_score(t_array,y_array,average='macro',zero_division=0)\n",
    "    avg_f1 = f1_score(t_array,y_array,average='macro',zero_division=0)\n",
    "\n",
    "    z_xy_array = np.array(z_xy_list)\n",
    "    z_y_array = np.array(z_y_list)\n",
    "    #z_plot(z_xy_array,t_array,'qz_xy',epoch,DIR_OUT)\n",
    "    #z_plot(z_y_array,t_array,'pz_y',epoch,DIR_OUT)\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_gauss, avg_Xent, avg_precision, avg_recall, avg_f1\n",
    "\n",
    "def test(K,pz_y,px_z,qy_x,qz_xy,dataloader,ALPHA,BETA,device):\n",
    "    #K=K+1\n",
    "    pz_y.eval()\n",
    "    px_z.eval()\n",
    "    qy_x.eval()\n",
    "    qz_xy.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = total_recon_loss = total_kl_gauss = total_Xent = 0.0\n",
    "        t_list = []\n",
    "        y_list = []\n",
    "\n",
    "        for x, t, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "        # ----- クロスエントロピー項\n",
    "            y = qy_x(x)\n",
    "            Xent = nn.functional.cross_entropy(y, t, reduction=\"sum\")\n",
    "        # ----- 再構性項\n",
    "            t_onehot = torch.eye(K)[t].to(device)\n",
    "            z = qz_xy(x, y)\n",
    "            recon_x = px_z(z)\n",
    "            recon_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "        # ----- Prior Network Loss\n",
    "            pz_y(t_onehot)\n",
    "            kl_gauss = KL_Gauss(qz_xy.mu, qz_xy.logvar, pz_y.mu, pz_y.logvar)\n",
    "\n",
    "            loss = recon_loss + kl_gauss*BETA + Xent*ALPHA\n",
    "            total_loss += loss\n",
    "            total_recon_loss += recon_loss\n",
    "            total_kl_gauss += kl_gauss\n",
    "            total_Xent += Xent\n",
    "\n",
    "            y = y.argmax(dim=1).detach().cpu().numpy()\n",
    "            t = t.detach().cpu().numpy()\n",
    "            y_list.extend(y)\n",
    "            t_list.extend(t)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(dataloader.dataset)\n",
    "    avg_kl_gauss = total_kl_gauss / len(dataloader.dataset)\n",
    "    avg_Xent = total_Xent / len(dataloader.dataset)\n",
    "    y_array = np.array(y_list)\n",
    "    t_array = np.array(t_list)\n",
    "    avg_precision = precision_score(t_array,y_array,average='macro',zero_division=0)\n",
    "    avg_recall = recall_score(t_array,y_array,average='macro',zero_division=0)\n",
    "    avg_f1 = f1_score(t_array,y_array,average='macro',zero_division=0)\n",
    "\n",
    "    return avg_loss, avg_recon_loss, avg_kl_gauss, avg_Xent, avg_precision, avg_recall, avg_f1\n",
    "\n",
    "def do_train_and_test(K,pz_y,px_z,qy_x,qz_xy,train_loader,test_loader,optimizer,epoch_num,device,ALPHA,BETA,DIR_OUT):\n",
    "    history = {}\n",
    "    history['train_loss'] = []\n",
    "    history['train_recon_loss'] = []\n",
    "    history['train_kl_gauss'] = []\n",
    "    history['train_Xent'] = []\n",
    "    history['train_precision'] = []\n",
    "    history['train_recall'] = []\n",
    "    history['train_f1'] = []\n",
    "\n",
    "    history['test_loss'] = []\n",
    "    history['test_recon_loss'] = []\n",
    "    history['test_kl_gauss'] = []\n",
    "    history['test_Xent'] = []\n",
    "    history['test_precision'] = []\n",
    "    history['test_recall'] = []\n",
    "    history['test_f1'] = []\n",
    "\n",
    "    for epoch in tqdm(range(epoch_num)):\n",
    "        train_loss,train_recon_loss,train_kl_gauss,train_Xent,train_precision,train_recall,train_f1 = train(K+1,pz_y,px_z,qy_x,qz_xy,train_loader,optimizer,ALPHA,BETA,device,epoch,DIR_OUT)\n",
    "        test_loss,test_recon_loss,test_kl_gauss,test_Xent,test_precision,test_recall,test_f1 = test(K+1,pz_y,px_z,qy_x,qz_xy,test_loader,ALPHA,BETA,device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_recon_loss'].append(train_recon_loss)\n",
    "        history['train_kl_gauss'].append(train_kl_gauss)\n",
    "        history['train_Xent'].append(train_Xent)\n",
    "        history['train_precision'].append(train_precision)\n",
    "        history['train_recall'].append(train_recall)\n",
    "        history['train_f1'].append(train_f1)\n",
    "\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_recon_loss'].append(test_recon_loss)\n",
    "        history['test_kl_gauss'].append(test_kl_gauss)\n",
    "        history['test_Xent'].append(test_Xent)\n",
    "        history['test_precision'].append(test_precision)\n",
    "        history['test_recall'].append(test_recall)\n",
    "        history['test_f1'].append(test_f1)\n",
    "    return history\n",
    "\n",
    "def plot_every_epoch(values1,values2,rng,label1,label2,ylabel,savefig_path,bbox_to_anchor=(1,1),loc='center right'):\n",
    "    fig = plt.figure(figsize=(2.5,2),dpi=300)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(range(rng),values1,label=label1)\n",
    "    ax.plot(range(rng),values2,label=label2)\n",
    "    ax.legend(bbox_to_anchor=bbox_to_anchor,loc=loc,borderaxespad=0)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(savefig_path,bbox_inches='tight',pad_inches=0.05)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "# ----- パラメータ設定\n",
    "    DISTANCE = args.distance\n",
    "    Z_DIM = args.z_dim\n",
    "    K = Y_DIM = args.y_dim\n",
    "    IS_SUPERVISED = args.disable_supervised\n",
    "    EPOCH_NUM = args.epoch_num #200\n",
    "    TAU = args.tau # 温度\n",
    "    BETA = args.beta #7 kl_gaussの係数\n",
    "    ALPHA = args.alpha #1000 Xentの係数\n",
    "    SEED = args.seed\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    LR = args.lr # 学習率\n",
    "    SMA_NUM = args.sma_num\n",
    "    TEST_PERSON_NUM = args.test_person_num\n",
    "    WAVELET_HEIGHT = args.wavelet_height\n",
    "    WAVELET_WIDTH = args.wavelet_width\n",
    "    PLOT_RECON = args.plot_recon\n",
    "    FREQS_START = args.freqs_start\n",
    "    FREQS_END = args.freqs_end\n",
    "    FS = args.fs\n",
    "    POOL = args.pool\n",
    "    YULE = args.yule\n",
    "\n",
    "# ----- データ入出力するディレクトリ\n",
    "    PARENT_DIR_OUT = args.pardir\n",
    "    if DISTANCE == None:\n",
    "        CHILD_DIR_OUT = 'sma'+str(SMA_NUM)+'_zdim'+str(Z_DIM)+'_pool'+str(POOL)+'_epoch'+str(EPOCH_NUM)+'_ydim'+str(Y_DIM)+'/'\n",
    "    else:\n",
    "        CHILD_DIR_OUT = 'sma'+str(SMA_NUM)+'_zdim'+str(Z_DIM)+'_pool'+str(POOL)+'_'+DISTANCE+'_epoch'+str(EPOCH_NUM)+'_ydim'+str(Y_DIM)+'/'\n",
    "    DIR_OUT = PARENT_DIR_OUT + CHILD_DIR_OUT\n",
    "\n",
    "    FILELIST_DIR = args.filelist_dir\n",
    "    FLISTNAME_IN = args.flistname_in\n",
    "    FILELIST_IN_PATH = FILELIST_DIR + FLISTNAME_IN\n",
    "\n",
    "    if not os.path.exists(PARENT_DIR_OUT):\n",
    "        os.mkdir(PARENT_DIR_OUT)\n",
    "    if not os.path.exists(DIR_OUT):\n",
    "        os.mkdir(DIR_OUT)\n",
    "\n",
    "    if PLOT_RECON and not os.path.exists(DIR_OUT+'figs/'):\n",
    "        os.mkdir(DIR_OUT+'figs/')\n",
    "\n",
    "    if not os.path.exists(FILELIST_DIR):\n",
    "        os.mkdir(FILELIST_DIR)\n",
    "\n",
    "\n",
    "    #K = Y_DIM = get_info_from_filelist.get_y_dim(FILELIST)\n",
    "    print(\"########## パラメータ ##########\")\n",
    "    print(f\"Z_DIM = {Z_DIM}\\nK = Y_DIM = {K}\\nIS_SUPERVISED = {IS_SUPERVISED}\\nepoch_num = {EPOCH_NUM}\\ntau = {TAU}\\nbeta = {BETA} kl_gaussの係数\\nalpha = {ALPHA} Xentの係数\\nbatch_size = {BATCH_SIZE}\\nlr = {LR}\\nsma_num = {SMA_NUM}\\nfs = {FS}\\npool = {POOL}\")\n",
    "    print(\"###############################\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "# ----- データセット作成\n",
    "    print('ファイル読み込み...')\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    #train_dataset, _test_dataset = createMydataset2D(Y_DIM,FILELIST_IN_PATH, SMA_NUM, WAVELET_HEIGHT, WAVELET_WIDTH, transform, DISTANCE, FREQS_START, FREQS_END)\n",
    "    #noise_dataset = createNoiseDataset(34*9,WAVELET_HEIGHT,WAVELET_WIDTH,transform)\n",
    "    #train_dataset = MyDataset(np.concatenate([train_dataset.data,noise_dataset.data],0).astype(np.float32),\n",
    "    #                    np.concatenate([train_dataset.label,noise_dataset.label],0),\n",
    "    #                    np.concatenate([train_dataset.datatime_idx,noise_dataset.datatime_idx],0).astype(np.float32))\n",
    "    #_, test_dataset = createMydataset2D(5,FILELIST_IN_PATH, SMA_NUM, WAVELET_HEIGHT, WAVELET_WIDTH, transform, DISTANCE, FREQS_START, FREQS_END)\n",
    "\n",
    "    train_dataset, test_dataset = createMydataset2D_addNoise(Y_DIM, TEST_PERSON_NUM, FILELIST_IN_PATH, SMA_NUM, WAVELET_HEIGHT, WAVELET_WIDTH, transform, DISTANCE, FREQS_START, FREQS_END)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# ----- ネットワークモデル\n",
    "    if YULE and FS == 125 and POOL == 2:\n",
    "        px_z = model_yule_pool2_125hz.Px_z(z_dim=Z_DIM).to(device)\n",
    "        pz_y = model_yule_pool2_125hz.Pz_y(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "        qy_x = model_yule_pool2_125hz.Qy_x(y_dim=Y_DIM).to(device)\n",
    "        qz_xy = model_yule_pool2_125hz.Qz_xy(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "    elif YULE and FS == 125 and POOL == 4:\n",
    "        px_z = model_yule_pool4_125hz.Px_z(z_dim=Z_DIM).to(device)\n",
    "        pz_y = model_yule_pool4_125hz.Pz_y(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "        qy_x = model_yule_pool4_125hz.Qy_x(y_dim=Y_DIM).to(device)\n",
    "        qz_xy = model_yule_pool4_125hz.Qz_xy(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "    elif YULE and FS == 500 and POOL == 2:\n",
    "        px_z = model_yule_pool2_500hz.Px_z(z_dim=Z_DIM).to(device)\n",
    "        pz_y = model_yule_pool2_500hz.Pz_y(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "        qy_x = model_yule_pool2_500hz.Qy_x(y_dim=Y_DIM).to(device)\n",
    "        qz_xy = model_yule_pool2_500hz.Qz_xy(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "    elif YULE and FS == 500 and POOL == 4:\n",
    "        px_z = model_yule_pool4_500hz.Px_z(z_dim=Z_DIM).to(device)\n",
    "        pz_y = model_yule_pool4_500hz.Pz_y(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "        qy_x = model_yule_pool4_500hz.Qy_x(y_dim=Y_DIM).to(device)\n",
    "        qz_xy = model_yule_pool4_500hz.Qz_xy(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "    elif FS == 125 and POOL == 2:\n",
    "        print(\"model\")\n",
    "        px_z = model_pool2_125hz.Px_z(z_dim=Z_DIM).to(device)\n",
    "        pz_y = model_pool2_125hz.Pz_y(z_dim=Z_DIM, y_dim=Y_DIM+1).to(device)\n",
    "        qy_x = model_pool2_125hz.Qy_x(y_dim=Y_DIM+1).to(device)\n",
    "        qz_xy = model_pool2_125hz.Qz_xy(z_dim=Z_DIM, y_dim=Y_DIM+1).to(device)\n",
    "    elif FS == 125 and POOL == 4:\n",
    "        px_z = model_pool4_125hz.Px_z(z_dim=Z_DIM).to(device)\n",
    "        pz_y = model_pool4_125hz.Pz_y(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "        qy_x = model_pool4_125hz.Qy_x(y_dim=Y_DIM).to(device)\n",
    "        qz_xy = model_pool4_125hz.Qz_xy(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "    elif FS == 500 and POOL == 2:\n",
    "        px_z = model_pool2_500hz.Px_z(z_dim=Z_DIM).to(device)\n",
    "        pz_y = model_pool2_500hz.Pz_y(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "        qy_x = model_pool2_500hz.Qy_x(y_dim=Y_DIM).to(device)\n",
    "        qz_xy = model_pool2_500hz.Qz_xy(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "    elif FS == 500 and POOL == 4:\n",
    "        px_z = model_pool4_500hz.Px_z(z_dim=Z_DIM).to(device)\n",
    "        pz_y = model_pool4_500hz.Pz_y(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "        qy_x = model_pool4_500hz.Qy_x(y_dim=Y_DIM).to(device)\n",
    "        qz_xy = model_pool4_500hz.Qz_xy(z_dim=Z_DIM, y_dim=Y_DIM).to(device)\n",
    "    else:\n",
    "        raise ModelError('nn.MaxPool2d()のカーネルサイズかサンプリングレート非対応')\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(px_z.parameters())+list(pz_y.parameters())\n",
    "        +list(qy_x.parameters())+list(qz_xy.parameters()), lr=LR)\n",
    "\n",
    "    #validation、一応準備してるけど使ってない\n",
    "    # train_size = int(len(train_dataset) * 0.9)\n",
    "    # val_size = len(train_dataset) - train_size\n",
    "    # train_dataset_s, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    # train_loader = DataLoader(\n",
    "    #     dataset=train_dataset_s, BATCH_SIZE=128, shuffle=True)\n",
    "    # test_loader = DataLoader(\n",
    "    #     dataset=val_dataset, BATCH_SIZE=128, shuffle=True)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    history = do_train_and_test(K,pz_y,px_z,qy_x,qz_xy,train_loader,test_loader,optimizer,EPOCH_NUM,device,ALPHA,BETA,DIR_OUT)\n",
    "    #values1,values2,rng,label1,label2,ylabel,savefig_path,bbox_to_anchor=(1,1),loc='center right'):\n",
    "    \"\"\"\n",
    "    plot_every_epoch(history['train_loss'],history['test_loss'],EPOCH_NUM,'train','test','Loss',DIR_OUT+'loss.svg',loc='upper right')\n",
    "    plot_every_epoch(history['train_recon_loss'],history['test_recon_loss'],EPOCH_NUM,'train','test','$\\mathcal{L}_{RC}$',DIR_OUT+'loss_rc.svg',loc='upper right')\n",
    "    plot_every_epoch(history['train_kl_gauss'],history['test_kl_gauss'],EPOCH_NUM,'train','test','$\\mathcal{L}_{KL}$',DIR_OUT+'loss_kl.svg',loc='upper right')\n",
    "    plot_every_epoch(history['train_Xent'],history['test_Xent'],EPOCH_NUM,'train','test','$\\mathcal{L}_{CE}$',DIR_OUT+'loss_ce.svg',loc='upper right')\n",
    "    plot_every_epoch(history['train_precision'],history['test_precision'],EPOCH_NUM,'train','test','Precision',DIR_OUT+'score_precision.svg',bbox_to_anchor=(1,0),loc='lower right')\n",
    "    plot_every_epoch(history['train_recall'],history['test_recall'],EPOCH_NUM,'train','test','Recall',DIR_OUT+'score_recall.svg',bbox_to_anchor=(1,0),loc='lower right')\n",
    "    plot_every_epoch(history['train_f1'],history['test_f1'],EPOCH_NUM,'train','test','F-measure',DIR_OUT+'score_f1.svg',bbox_to_anchor=(1,0),loc='lower right')\n",
    "\n",
    "    plot_every_epoch(history['train_loss'],history['test_loss'],EPOCH_NUM,'train','test','Loss',DIR_OUT+'loss.png',loc='upper right')\n",
    "    plot_every_epoch(history['train_recon_loss'],history['test_recon_loss'],EPOCH_NUM,'train','test','$\\mathcal{L}_{RC}$',DIR_OUT+'loss_rc.png',loc='upper right')\n",
    "    plot_every_epoch(history['train_kl_gauss'],history['test_kl_gauss'],EPOCH_NUM,'train','test','$\\mathcal{L}_{KL}$',DIR_OUT+'loss_kl.png',loc='upper right')\n",
    "    plot_every_epoch(history['train_Xent'],history['test_Xent'],EPOCH_NUM,'train','test','$\\mathcal{L}_{CE}$',DIR_OUT+'loss_ce.png',loc='upper right')\n",
    "    plot_every_epoch(history['train_precision'],history['test_precision'],EPOCH_NUM,'train','test','Precision',DIR_OUT+'score_precision.png',bbox_to_anchor=(1,0),loc='lower right')\n",
    "    plot_every_epoch(history['train_recall'],history['test_recall'],EPOCH_NUM,'train','test','Recall',DIR_OUT+'score_recall.png',bbox_to_anchor=(1,0),loc='lower right')\n",
    "    plot_every_epoch(history['train_f1'],history['test_f1'],EPOCH_NUM,'train','test','F-measure',DIR_OUT+'score_f1.png',bbox_to_anchor=(1,0),loc='lower right')\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(4.5,2),dpi=300)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(range(EPOCH_NUM),history['train_loss'],label='Train Loss')\n",
    "    ax.plot(range(EPOCH_NUM),history['test_loss'],label='Test Loss',color='tab:blue',linestyle=':')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    plt.grid(b=False)\n",
    "    handle1, label1 = ax.get_legend_handles_labels()\n",
    "    ax = ax.twinx()\n",
    "    ax.plot(range(EPOCH_NUM),history['train_f1'],label='Train Accuracy',color='tab:orange')\n",
    "    ax.plot(range(EPOCH_NUM),history['test_f1'],label='Test Accuracy',color='tab:orange',linestyle=':')\n",
    "    ax.spines['right'].set_color('tab:orange')\n",
    "    ax.tick_params(axis='y',colors='tab:orange')\n",
    "    handle2, label2 = ax.get_legend_handles_labels()\n",
    "    ax.legend(handle1+handle2,label1+label2,bbox_to_anchor=(1.2,0.5),loc=6)\n",
    "    ax.set_ylabel('Accuracy',color='tab:orange')\n",
    "    fig.tight_layout()\n",
    "    plt.grid(b=False)\n",
    "    plt.savefig(DIR_OUT+'loss_acc.png',bbox_inches='tight',pad_inches=0.05)\n",
    "    plt.savefig(DIR_OUT+'loss_acc.svg',bbox_inches='tight',pad_inches=0.05)\n",
    "    subprocess.call('inkscape -M '+DIR_OUT+'loss_acc.emf '+DIR_OUT+'loss_acc.svg',shell=True)\n",
    "    plt.close()\n",
    "    #lossをプロット＆ファイル出力したいので準備する。\n",
    "    \"\"\"\n",
    "    t = np.arange(0,EPOCH_NUM)\n",
    "    loss_list = []\n",
    "    recon_loss_list = []\n",
    "    kl_cat_list = []\n",
    "    kl_gauss_list = []\n",
    "    if IS_SUPERVISED:\n",
    "        Xent_list = []\n",
    "\n",
    "    for epoch in tqdm(range(EPOCH_NUM)):\n",
    "\n",
    "        if IS_SUPERVISED:\n",
    "            loss = train(K,pz_y,px_z,qy_x,qz_xy,train_loader,loss_list,recon_loss_list,kl_gauss_list,Xent_list,optimizer,ALPHA,BETA,device)\n",
    "\n",
    "\n",
    "        else:\n",
    "            for x, _ in train_loader:\n",
    "                x = x.to(device)\n",
    "                pi = qy_x(x)\n",
    "                y = gumbel_softmax_sampling(pi, shape=pi.shape, tau=TAU)\n",
    "                z = qz_xy(x, y)\n",
    "                recon_x = px_z(z)\n",
    "                recon_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "\n",
    "                pi_prior = (torch.ones(K)/K).to(device)\n",
    "                kl_cat = KL_Cat(pi, pi_prior)\n",
    "                pz_y(y)\n",
    "                kl_gauss = KL_Gauss(qz_xy.mu, qz_xy.logvar, pz_y.mu, pz_y.logvar)\n",
    "                loss = recon_loss + kl_cat + kl_gauss*BETA\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_list.append(loss)\n",
    "            recon_loss_list.append(recon_loss)\n",
    "            kl_cat_list.append(kl_cat)\n",
    "            kl_gauss_list.append(kl_gauss)\n",
    "\n",
    "    #loss plot\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    if IS_SUPERVISED:\n",
    "        ax = fig.add_subplot(141)\n",
    "        ax.plot(t,loss_list)\n",
    "        ax.set_title(\"Loss\")\n",
    "        ax = fig.add_subplot(142)\n",
    "        ax.plot(t,recon_loss_list)\n",
    "        ax.set_title(\"recon_loss\")\n",
    "        ax = fig.add_subplot(143)\n",
    "        ax.plot(t,kl_gauss_list)\n",
    "        ax.set_title(\"kl_gauss\")\n",
    "        ax = fig.add_subplot(144)\n",
    "        ax.plot(t,Xent_list)\n",
    "        ax.set_title(\"cross_entropy\")\n",
    "        loss_ndarray = np.vstack((np.array(loss_list),np.array(recon_loss_list),np.array(kl_gauss_list),np.array(Xent_list)))\n",
    "    else:\n",
    "        ax = fig.add_subplot(141)\n",
    "        ax.plot(t,loss_list)\n",
    "        ax.set_title(\"Loss\")\n",
    "        ax = fig.add_subplot(142)\n",
    "        ax.plot(t,recon_loss_list)\n",
    "        ax.set_title(\"recon_loss\")\n",
    "        ax = fig.add_subplot(143)\n",
    "        ax.plot(t,kl_cat_list)\n",
    "        ax.set_title(\"kl_cat\")\n",
    "        ax = fig.add_subplot(144)\n",
    "        ax.plot(t,kl_gauss_list)\n",
    "        ax.set_title(\"kl_gauss\")\n",
    "        loss_ndarray = np.vstack((np.array(loss_list),np.array(recon_loss_list),np.array(kl_cat_list),np.array(kl_gauss_list)))\n",
    "    fig.savefig(DIR_OUT+'loss_2d_sma'+str(SMA_NUM)+'.png')\n",
    "    np.savetxt(DIR_OUT+'loss_sma'+str(SMA_NUM)+'.csv',loss_ndarray,delimiter=',')\"\"\"\n",
    "\n",
    "    ### 12/04追記 推論モードでcpu使う ###\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    #学習結果を一旦保存\n",
    "    torch.save(px_z.state_dict(), DIR_OUT+'px_z.pth')\n",
    "    torch.save(pz_y.state_dict(), DIR_OUT+'pz_y.pth')\n",
    "    torch.save(qy_x.state_dict(), DIR_OUT+'qy_x.pth')\n",
    "    torch.save(qz_xy.state_dict(), DIR_OUT+'qz_xy.pth')\n",
    "\n",
    "    #学習結果を読み込んで、\n",
    "    px_z.load_state_dict(torch.load(DIR_OUT+'px_z.pth', map_location=lambda storage, loc: storage))\n",
    "    pz_y.load_state_dict(torch.load(DIR_OUT+'pz_y.pth', map_location=lambda storage, loc: storage))\n",
    "    qy_x.load_state_dict(torch.load(DIR_OUT+'qy_x.pth', map_location=lambda storage, loc: storage))\n",
    "    qz_xy.load_state_dict(torch.load(DIR_OUT+'qz_xy.pth', map_location=lambda storage, loc: storage))\n",
    "    #推論モードに設定（Dropout、BN、他の無効化）.to(device)は12/04追加。\n",
    "    px_z.eval().to(device)\n",
    "    pz_y.train().to(device)\n",
    "    qy_x.eval().to(device)\n",
    "    qz_xy.train().to(device)\n",
    "    print(f\"qz_xy.training:{qz_xy.training}(trueなら学習モード/falseなら推論モード)\")\n",
    "\n",
    "    colors = plt.cm.get_cmap('hsv')\n",
    "    colors20 = plt.cm.get_cmap('tab20')\n",
    "    cmap = cm.get_cmap('hsv',21)\n",
    "    color_code = []\n",
    "    for i in range(cmap.N):\n",
    "        rgb = cmap(i)[:3]\n",
    "        color_code.append(rgb2hex(rgb))\n",
    "\n",
    "    cmap = ListedColormap(color_code)\n",
    "    bounds = list(range(0,21))\n",
    "    #norm = BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    # ノイズ用に1クラス追加\n",
    "    K += 1\n",
    "    Y_DIM += 1\n",
    "\n",
    "    ltrain_pred = []\n",
    "    ztrain_pred = []\n",
    "    train_datatime_idx = []\n",
    "    #2021.2.5追記 qy_xによる予測も出力\n",
    "    ltrain_qy_x = []\n",
    "    ztrain_pz_y = []\n",
    "    #count=0\n",
    "    for x, t, datatime_idx in train_loader:\n",
    "        #print(datatime_idx.detach().cpu().numpy())\n",
    "        x = x.to(device)\n",
    "        y = qy_x(x)\n",
    "        with torch.no_grad():\n",
    "            z = qz_xy(x, y)\n",
    "        y_label_onehot = torch.eye(K)[t].to(device)\n",
    "        with torch.no_grad():\n",
    "            pz_y_out = pz_y(y_label_onehot) #この分布にqz_xyを近づけようと学習していたはず\n",
    "\n",
    "        #print(f\"y[0]:{y[0]}\")\n",
    "        if PLOT_RECON:\n",
    "            recon_x_train = px_z(z) # reconstruction\n",
    "            #print(f\"recon_x_train.shape : {recon_x_train.shape}\")\n",
    "            #reconst_plot(x,recon_x_train,DIR_OUT+'figs/',125,WAVELET_HEIGHT,WAVELET_WIDTH,BATCH_SIZE,count,mode='train')\n",
    "            reconst_plot(x,recon_x_train,DIR_OUT+'figs/',125,WAVELET_HEIGHT,WAVELET_WIDTH,datatime_idx,mode='train')\n",
    "        ztrain_pred.extend(z.detach().cpu().numpy())\n",
    "        ltrain_pred.extend(t.numpy())\n",
    "        train_datatime_idx.extend(datatime_idx.numpy())\n",
    "        ltrain_qy_x.extend(y.argmax(dim=1).numpy()) #2021.2.5\n",
    "        ztrain_pz_y.extend(pz_y_out.detach().cpu().numpy())\n",
    "        #count += 1\n",
    "    ltrain_pred = np.array(ltrain_pred)\n",
    "    ztrain_pred = np.array(ztrain_pred)\n",
    "    train_datatime_idx = np.array(train_datatime_idx)\n",
    "    ltrain_qy_x = np.array(ltrain_qy_x)\n",
    "    ztrain_pz_y = np.array(ztrain_pz_y)\n",
    "\n",
    "    ltest_pred = []\n",
    "    ztest_pred = []\n",
    "    test_datatime_idx = []\n",
    "    #2021.2.5追記 qy_xによる予測も出力\n",
    "    ltest_qy_x = []\n",
    "    count=0\n",
    "    for x, t, datatime_idx in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = qy_x(x)\n",
    "        with torch.no_grad():\n",
    "            z = qz_xy(x, y)\n",
    "        if PLOT_RECON:\n",
    "            recon_x_test = px_z(z)\n",
    "            #reconst_plot(x,recon_x_test,DIR_OUT+'figs/',125,WAVELET_HEIGHT,WAVELET_WIDTH,BATCH_SIZE,count,mode='test')\n",
    "            reconst_plot(x,recon_x_test,DIR_OUT+'figs/',125,WAVELET_HEIGHT,WAVELET_WIDTH,datatime_idx,mode='test')\n",
    "        ztest_pred.extend(z.detach().cpu().numpy())\n",
    "        ltest_pred.extend(t.numpy())\n",
    "        test_datatime_idx.extend(datatime_idx.numpy())\n",
    "        ltest_qy_x.extend(y.argmax(dim=1).numpy()) #2021.2.5\n",
    "        count += 1\n",
    "    ltest_pred = np.array(ltest_pred)\n",
    "    ztest_pred = np.array(ztest_pred)\n",
    "    test_datatime_idx = np.array(test_datatime_idx)\n",
    "    ltest_qy_x = np.array(ltest_qy_x)\n",
    "\n",
    "    test_accuracy = accuracy_score(ltest_pred,ltest_qy_x)\n",
    "    if not os.path.isfile(DIR_OUT+'accuracy_score.csv'):\n",
    "        with open(DIR_OUT+'accuracy_score.csv',mode='w',newline='') as f:\n",
    "            dt_now = datetime.now()\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([str(test_accuracy), dt_now.strftime('%Y-%m-%d %H:%M:%S')])\n",
    "    else:\n",
    "        with open(DIR_OUT+'accuracy_score.csv',mode='a',newline='') as f:\n",
    "            dt_now = datetime.now()\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([str(test_accuracy), dt_now.strftime('%Y-%m-%d %H:%M:%S')])\n",
    "\n",
    "# ----- 2021.2.5 p(z|y)の分布をプロット\n",
    "    fig = plt.figure(num=None, figsize=(4, 3), dpi=300, facecolor='w', edgecolor='k')\n",
    "    subxy = 2 if Z_DIM > 2 else 1\n",
    "    ax = plt.subplot(subxy, subxy, 1)\n",
    "    for i in range(Y_DIM):\n",
    "        zplot = ztrain_pz_y[ltrain_pred == i]\n",
    "        #ax.scatter(zplot[:, 0], zplot[:, 1], s=5, label=i, color=colors(2*i/(Y_DIM*2)))\n",
    "        #sc = ax.scatter(zplot[:, 0], zplot[:, 1], s=5, c=c, cmap=colors20,norm=norm,vmin=min(bounds),vmax=max(bounds))\n",
    "        num = zplot.shape[0]\n",
    "        c = np.full(num,i)\n",
    "        sc = ax.scatter(zplot[:, 0], zplot[:, 1], s=5, c=c, cmap=colors20,norm=Normalize(vmin=0,vmax=19))\n",
    "        #zplot = np.vstack((zplot[:,0],zplot[:,1])).T\n",
    "        #zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "        #ax.annotate(str(i),zavg)\n",
    "        #zplot = ztest_pred[ltest_pred == i]\n",
    "        #ax.scatter(zplot[:, 0], zplot[:, 1], s=5, label=i, c=colors[i + Y_DIM])\n",
    "        #plt.legend()\n",
    "    ax.set_xlim(-4.5,4.5)\n",
    "    ax.set_ylim(-4.5,4.5)\n",
    "    ax.set_aspect('equal',adjustable='box')\n",
    "    cbar = fig.colorbar(sc,aspect=30)\n",
    "    cbar.ax.tick_params(direction='out')\n",
    "    cbar.ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    if Z_DIM > 2:\n",
    "        ax = plt.subplot(subxy, subxy, 2)\n",
    "        for i in range(Y_DIM):\n",
    "            zplot = ztrain_pz_y[ltrain_pred == i]\n",
    "            ax.scatter(zplot[:, 0], zplot[:, 2], s=5, label=i,color=colors(2*i/(Y_DIM*2)))\n",
    "            zplot = np.vstack((zplot[:,0],zplot[:,2])).T\n",
    "            zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "            ax.annotate(str(i),zavg)\n",
    "            #zplot = ztest_pred[ltest_pred == i]\n",
    "            #ax.scatter(zplot[:, 0], zplot[:, 2], s=5, label=i, c=colors[i + Y_DIM])\n",
    "            plt.legend()\n",
    "        ax = plt.subplot(subxy, subxy, 3)\n",
    "        for i in range(Y_DIM):\n",
    "            zplot = ztrain_pz_y[ltrain_pred == i]\n",
    "            ax.scatter(zplot[:, 1], zplot[:, 2], s=5, label=i, color=colors(2*i/(Y_DIM*2)))\n",
    "            zplot = np.vstack((zplot[:,1],zplot[:,2])).T\n",
    "            zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "            ax.annotate(str(i),zavg)\n",
    "            #zplot = ztest_pred[ltest_pred == i]\n",
    "            #ax.scatter(zplot[:, 1], zplot[:, 2], s=5, label=i, c=colors[i + Y_DIM])\n",
    "            plt.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(DIR_OUT+\"Qz_y.png\")\n",
    "    \"\"\"\n",
    "    fig = plt.figure(num=None, figsize=(3, 3), dpi=300, facecolor='w', edgecolor='k')\n",
    "    subxy = 2 if Z_DIM > 2 else 1\n",
    "    print(f\"Z_DIM:{Z_DIM}, subxy:{subxy}\")\n",
    "    ax = plt.subplot(subxy, subxy, 1)\n",
    "    for i in range(Y_DIM):\n",
    "        zplot = ztrain_pred[ltrain_pred == i]\n",
    "        ax.scatter(zplot[:, 0], zplot[:, 1], s=5, label=i, color=colors(2*i/(Y_DIM*2)))\n",
    "        zplot = np.vstack((zplot[:,0],zplot[:,1])).T\n",
    "        zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "        ax.annotate(str(i),zavg)\n",
    "        zplot = ztest_pred[ltest_pred == i]\n",
    "        ax.scatter(zplot[:, 0], zplot[:, 1], s=5, label=i, color=colors((2*i+1)/(Y_DIM*2)))\n",
    "        #plt.legend()\n",
    "    if Z_DIM > 2:\n",
    "        ax = plt.subplot(subxy, subxy, 2)\n",
    "        for i in range(Y_DIM):\n",
    "            zplot = ztrain_pred[ltrain_pred == i]\n",
    "            ax.scatter(zplot[:, 0], zplot[:, 2], s=5, label=i, color=colors(2*i/(Y_DIM*2)))\n",
    "            zplot = np.vstack((zplot[:,0],zplot[:,2])).T\n",
    "            zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "            ax.annotate(str(i),zavg)\n",
    "            zplot = ztest_pred[ltest_pred == i]\n",
    "            ax.scatter(zplot[:, 0], zplot[:, 2], s=5, label=i, color=colors((2*i+1)/(Y_DIM*2)))\n",
    "            plt.legend()\n",
    "        ax = plt.subplot(subxy, subxy, 3)\n",
    "        for i in range(Y_DIM):\n",
    "            zplot = ztrain_pred[ltrain_pred == i]\n",
    "            ax.scatter(zplot[:, 1], zplot[:, 2], s=5, label=i, color=colors(2*i/(Y_DIM*2)))\n",
    "            zplot = np.vstack((zplot[:,1],zplot[:,2])).T\n",
    "            zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "            ax.annotate(str(i),zavg)\n",
    "            zplot = ztest_pred[ltest_pred == i]\n",
    "            ax.scatter(zplot[:, 1], zplot[:, 2], s=5, label=i, color=colors((2*i+1)/(Y_DIM*2)))\n",
    "            plt.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(DIR_OUT+\"VAEclustering_2d_sma\"+str(SMA_NUM)+\".png\")\"\"\"\n",
    "\n",
    "    fig2 = plt.figure(num=None, figsize=(4, 3), dpi=300, facecolor='w', edgecolor='k')\n",
    "    subxy = 2 if Z_DIM > 2 else 1\n",
    "    ax = plt.subplot(subxy, subxy, 1)\n",
    "    for i in range(Y_DIM):\n",
    "        zplot = ztrain_pred[ltrain_pred == i]\n",
    "        \"\"\"ax.scatter(zplot[:, 0], zplot[:, 1], s=5, label=i, color=colors(2*i/(Y_DIM*2)))\n",
    "        zplot = np.vstack((zplot[:,0],zplot[:,1])).T\n",
    "        zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "        ax.annotate(str(i),zavg)\"\"\"\n",
    "        num = zplot.shape[0]\n",
    "        c = np.full(num,i)\n",
    "        sc = ax.scatter(zplot[:, 0], zplot[:, 1], s=5, c=c, cmap=colors20,norm=Normalize(vmin=0,vmax=19))\n",
    "        #zplot = ztest_pred[ltest_pred == i]\n",
    "        #ax.scatter(zplot[:, 0], zplot[:, 1], s=5, label=i, c=colors[i + Y_DIM])\n",
    "        #plt.legend()\n",
    "    ax.set_xlim(-4.5,4.5)\n",
    "    ax.set_ylim(-4.5,4.5)\n",
    "    ax.set_aspect('equal',adjustable='box')\n",
    "    cbar = fig2.colorbar(sc,aspect=30)\n",
    "    cbar.ax.tick_params(direction='out')\n",
    "    cbar.ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    if Z_DIM > 2:\n",
    "        ax = plt.subplot(subxy, subxy, 2)\n",
    "        for i in range(Y_DIM):\n",
    "            zplot = ztrain_pred[ltrain_pred == i]\n",
    "            ax.scatter(zplot[:, 0], zplot[:, 2], s=5, label=i,color=colors(2*i/(Y_DIM*2)))\n",
    "            zplot = np.vstack((zplot[:,0],zplot[:,2])).T\n",
    "            zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "            ax.annotate(str(i),zavg)\n",
    "            #zplot = ztest_pred[ltest_pred == i]\n",
    "            #ax.scatter(zplot[:, 0], zplot[:, 2], s=5, label=i, c=colors[i + Y_DIM])\n",
    "            plt.legend()\n",
    "        ax = plt.subplot(subxy, subxy, 3)\n",
    "        for i in range(Y_DIM):\n",
    "            zplot = ztrain_pred[ltrain_pred == i]\n",
    "            ax.scatter(zplot[:, 1], zplot[:, 2], s=5, label=i, color=colors(2*i/(Y_DIM*2)))\n",
    "            zplot = np.vstack((zplot[:,1],zplot[:,2])).T\n",
    "            zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "            ax.annotate(str(i),zavg)\n",
    "            #zplot = ztest_pred[ltest_pred == i]\n",
    "            #ax.scatter(zplot[:, 1], zplot[:, 2], s=5, label=i, c=colors[i + Y_DIM])\n",
    "            plt.legend()\n",
    "    fig2.tight_layout()\n",
    "    fig2.savefig(DIR_OUT+\"VAEclustering_2d_train_sma\"+str(SMA_NUM)+\".png\")\n",
    "\n",
    "    fig3 = plt.figure(num=None, figsize=(4, 3), dpi=300, facecolor='w', edgecolor='k')\n",
    "    subxy = 2 if Z_DIM > 2 else 1\n",
    "    #print(f\"Y_DIM{Z_DIM}, subxy{subxy}\")\n",
    "    ax = plt.subplot(subxy, subxy, 1)\n",
    "    for i in range(Y_DIM):\n",
    "        #zplot = ztrain_pred[ltrain_pred == i]\n",
    "        #ax.scatter(zplot[:, 0], zplot[:, 1], s=5, label=i, c=colors[i])\n",
    "        zplot = ztest_pred[ltest_pred == i]\n",
    "        \"\"\"ax.scatter(zplot[:, 0], zplot[:, 1], s=5, label=i, color=colors((2*i+1)/(Y_DIM*2)))\n",
    "        zplot = np.vstack((zplot[:,0],zplot[:,1])).T\n",
    "        zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "        ax.annotate(str(i),zavg)\"\"\"\n",
    "        num = zplot.shape[0]\n",
    "        c = np.full(num,i)\n",
    "        sc = ax.scatter(zplot[:, 0], zplot[:, 1], s=5, c=c, cmap=colors20,norm=Normalize(vmin=0,vmax=19))\n",
    "    ax.set_xlim(-4.5,4.5)\n",
    "    ax.set_ylim(-4.5,4.5)\n",
    "    ax.set_aspect('equal',adjustable='box')\n",
    "    cbar = fig3.colorbar(sc,aspect=30)\n",
    "    cbar.ax.tick_params(direction='out')\n",
    "    cbar.ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "        #plt.legend()\n",
    "    if Z_DIM > 2:\n",
    "        ax = plt.subplot(subxy, subxy, 2)\n",
    "        for i in range(Y_DIM):\n",
    "            #zplot = ztrain_pred[ltrain_pred == i]\n",
    "            #ax.scatter(zplot[:, 0], zplot[:, 2], s=5, label=i, c=colors[i])\n",
    "            zplot = ztest_pred[ltest_pred == i]\n",
    "            ax.scatter(zplot[:, 0], zplot[:, 2], s=5, label=i, color=colors((2*i+1)/(Y_DIM*2)))\n",
    "            zplot = np.vstack((zplot[:,0],zplot[:,2])).T\n",
    "            zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "            ax.annotate(str(i),zavg)\n",
    "            plt.legend()\n",
    "        ax = plt.subplot(subxy, subxy, 3)\n",
    "        for i in range(Y_DIM):\n",
    "            #zplot = ztrain_pred[ltrain_pred == i]\n",
    "            #ax.scatter(zplot[:, 1], zplot[:, 2], s=5, label=i, c=colors[i])\n",
    "            zplot = ztest_pred[ltest_pred == i]\n",
    "            ax.scatter(zplot[:, 1], zplot[:, 2], s=5, label=i, color=colors((2*i+1)/(Y_DIM*2)))\n",
    "            zplot = np.vstack((zplot[:,1],zplot[:,2])).T\n",
    "            zavg = zplot.sum(axis=0)/zplot.shape[0]\n",
    "            ax.annotate(str(i),zavg)\n",
    "            plt.legend()\n",
    "    fig3.tight_layout()\n",
    "    fig3.savefig(DIR_OUT+\"VAEclustering_2d_test_sma\"+str(SMA_NUM)+\".png\")\n",
    "    #plt.show()\n",
    "\n",
    "    output_fp = open(DIR_OUT+\"result_train.csv\", 'w')\n",
    "    for label_i, latent_i, datatime_i, label_pred in zip(ltrain_pred, ztrain_pred, train_datatime_idx, ltrain_qy_x):\n",
    "        output_fp.write(str(label_i))\n",
    "        output_fp.write(\",\")\n",
    "        output_fp.write(str(label_pred))\n",
    "        output_fp.write(\",\")\n",
    "        for latent_val in latent_i:\n",
    "            output_fp.write(str(latent_val))\n",
    "            output_fp.write(\",\")\n",
    "        output_fp.write(str(datatime_i))\n",
    "        output_fp.write(\"\\n\")\n",
    "    output_fp.close()\n",
    "\n",
    "    output_fp = open(DIR_OUT+\"result_test.csv\", 'w')\n",
    "    for label_i, latent_i, datatime_i, label_pred in zip(ltest_pred, ztest_pred, test_datatime_idx, ltest_qy_x):\n",
    "        output_fp.write(str(label_i))\n",
    "        output_fp.write(\",\")\n",
    "        output_fp.write(str(label_pred))\n",
    "        output_fp.write(\",\")\n",
    "        for latent_val in latent_i:\n",
    "            output_fp.write(str(latent_val))\n",
    "            output_fp.write(\",\")\n",
    "        output_fp.write(str(datatime_i))\n",
    "        output_fp.write(\"\\n\")\n",
    "    output_fp.close()\n",
    "    print(DIR_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--distance DISTANCE] [--y_dim Y_DIM] [--z_dim Z_DIM] [--epoch_num EPOCH_NUM]\n",
      "                             [--tau TAU] [--beta BETA] [--alpha ALPHA] [--seed SEED] [--batch_size BATCH_SIZE]\n",
      "                             [--lr LR] [--sma_num SMA_NUM] [--test_person_num TEST_PERSON_NUM] [--fs FS]\n",
      "                             [--pool POOL] [--freqs_start FREQS_START] [--freqs_end FREQS_END]\n",
      "                             [--disable_supervised] [--plot_recon] [--yule] [--wavelet_height WAVELET_HEIGHT]\n",
      "                             [--wavelet_width WAVELET_WIDTH] [--pardir PARDIR] [--filelist_dir FILELIST_DIR]\n",
      "                             [--flistname_in FLISTNAME_IN]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\manabe ayumu\\AppData\\Roaming\\jupyter\\runtime\\kernel-b19e31e2-bb6e-4e46-9881-92fadcc351da.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--distance\", default='75cm')\n",
    "parser.add_argument(\"--y_dim\", type=int, default=5)\n",
    "parser.add_argument(\"--z_dim\", type=int, default=2)\n",
    "parser.add_argument(\"--epoch_num\", type=int, default=50)\n",
    "parser.add_argument(\"--tau\", type=float, default=0.5)#温度\n",
    "parser.add_argument(\"--beta\", type=float, default=10)\n",
    "parser.add_argument(\"--alpha\", type=float, default=2000)\n",
    "parser.add_argument(\"--seed\", type=int, default=1)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--sma_num\", type=int, default=5)\n",
    "parser.add_argument(\"--test_person_num\", type=int, default=1)\n",
    "\n",
    "parser.add_argument(\"--fs\", type=int, default=125)\n",
    "parser.add_argument(\"--pool\", type=int, default=2)\n",
    "\n",
    "parser.add_argument(\"--freqs_start\", type=int, default=3) #None\n",
    "parser.add_argument(\"--freqs_end\", type=int, default=51) #None\n",
    "\n",
    "parser.add_argument(\"--disable_supervised\", action='store_false')#デフォTrue\n",
    "parser.add_argument(\"--plot_recon\", action='store_true')#デフォFalse\n",
    "parser.add_argument(\"--yule\", action='store_true')\n",
    "\n",
    "parser.add_argument(\"--wavelet_height\", type=int, default=48) #切り取らないなら64\n",
    "parser.add_argument(\"--wavelet_width\", type=int, default=74)\n",
    "\n",
    "parser.add_argument(\"--pardir\", default='6_ClusteringResults/')\n",
    "parser.add_argument(\"--filelist_dir\", default='0_FileList/')\n",
    "parser.add_argument(\"--flistname_in\", default='4C_LogSma5Wavelet_125Hz.txt')\n",
    "\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
